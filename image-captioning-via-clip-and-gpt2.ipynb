{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install -q --upgrade \"pyarrow>=21.0.0\"\n!pip install -q \"pydantic>=2.0,<2.12\"\n\n!pip install -q transformers ftfy regex tqdm\n!pip install -q git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:00:13.689352Z","iopub.execute_input":"2025-10-13T12:00:13.689866Z","iopub.status.idle":"2025-10-13T12:01:47.197Z","shell.execute_reply.started":"2025-10-13T12:00:13.689846Z","shell.execute_reply":"2025-10-13T12:01:47.196234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import clip\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup # GPT2LMHeadModel: ph·∫ßn sinh ng√¥n ng·ªØ\nfrom torch.optim import AdamW\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport os\nimport requests\nimport random\nimport pandas as pd\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:01:47.199129Z","iopub.execute_input":"2025-10-13T12:01:47.199338Z","iopub.status.idle":"2025-10-13T12:02:11.461273Z","shell.execute_reply.started":"2025-10-13T12:01:47.19932Z","shell.execute_reply":"2025-10-13T12:02:11.460628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\ntokenizer.add_special_tokens({\"pad_token\": \"[PAD]\", \"bos_token\": \"<|startoftext|>\", \"eos_token\": \"<|endoftext|>\"})\ngpt2_model.resize_token_embeddings(len(tokenizer)) # Resize model embeddings\n\nfor param in clip_model.parameters():\n    param.requires_grad = False\n    \nfor param in gpt2_model.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:02:11.461957Z","iopub.execute_input":"2025-10-13T12:02:11.462469Z","iopub.status.idle":"2025-10-13T12:02:32.324908Z","shell.execute_reply.started":"2025-10-13T12:02:11.462438Z","shell.execute_reply":"2025-10-13T12:02:32.324014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nIMAGE_DIR = \"/kaggle/input/d/adityajn105/flickr8k/Images\"\nCAPTIONS_FILE = \"/kaggle/input/d/adityajn105/flickr8k/captions.txt\"\n\ndf = pd.read_csv(CAPTIONS_FILE)\nprint(f\"Total captions: {len(df)}\")\n\nimage_to_captions = defaultdict(list)\nfor index, row in df.iterrows():\n    image_name, caption = row['image'], row['caption']\n    image_to_captions[image_name].append(caption)\n\nall_images = list(image_to_captions.keys())\nprint(f\"Total unique images: {len(all_images)}\")\n\ntrain_size = int(0.8 * len(all_images))\nval_size = int(0.1 * len(all_images))\n\ntrain_images = all_images[:train_size]\nval_images = all_images[train_size:train_size + val_size]\ntest_images = all_images[train_size + val_size:]\n\nprint(f\"Train images: {len(train_images)}, Val images: {len(val_images)}, Test images: {len(test_images)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:02:32.32579Z","iopub.execute_input":"2025-10-13T12:02:32.326144Z","iopub.status.idle":"2025-10-13T12:02:33.825397Z","shell.execute_reply.started":"2025-10-13T12:02:32.326125Z","shell.execute_reply":"2025-10-13T12:02:33.824709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, image_keys, image_to_captions, tokenizer, preprocess, train=True):\n        self.image_keys = image_keys\n        self.image_to_captions = image_to_captions\n        self.tokenizer = tokenizer\n        self.preprocess = preprocess\n        self.max_len = 40 \n        self.train = train\n\n    def __len__(self):\n        return len(self.image_keys)\n\n    def __getitem__(self, idx):\n        image_key = self.image_keys[idx]\n        image_path = os.path.join(IMAGE_DIR, image_key)\n        \n        if self.train:\n            caption = random.choice(self.image_to_captions[image_key])\n        else:\n            caption = self.image_to_captions[image_key][0]\n            \n        image = Image.open(image_path).convert(\"RGB\")\n        image_processed = self.preprocess(image)\n        \n        caption_tokens = self.tokenizer(\n            f\"<|startoftext|>{caption}<|endoftext|>\",\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return image_processed, caption_tokens, image_path\n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:02:33.826116Z","iopub.execute_input":"2025-10-13T12:02:33.826346Z","iopub.status.idle":"2025-10-13T12:02:33.832618Z","shell.execute_reply.started":"2025-10-13T12:02:33.826323Z","shell.execute_reply":"2025-10-13T12:02:33.832014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MappingNetwork(nn.Module):\n\n    def __init__(self, clip_embedding_dim: int, gpt_embedding_dim: int, \n                 prefix_length: int = 10, num_layers: int = 8, num_heads: int = 8):\n        super().__init__()\n        self.prefix_length = prefix_length\n\n        self.prefix_const = nn.Parameter(torch.randn(1, prefix_length, gpt_embedding_dim))\n\n        self.clip_projection = nn.Linear(clip_embedding_dim, gpt_embedding_dim)\n\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=gpt_embedding_dim,\n            nhead=num_heads\n        )\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n\n    def forward(self, clip_embedding: torch.Tensor) -> torch.Tensor:\n        batch_size = clip_embedding.shape[0]\n    \n        clip_memory = self.clip_projection(clip_embedding).unsqueeze(0)  # [1, batch, dim]\n    \n        input_prefix = self.prefix_const.expand(batch_size, -1, -1)       # [batch, prefix_len, dim]\n        input_prefix = input_prefix.permute(1, 0, 2)                      # [prefix_len, batch, dim]\n    \n        output_prefix = self.transformer_decoder(tgt=input_prefix, memory=clip_memory)\n    \n        return output_prefix.permute(1, 0, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:02:33.851852Z","iopub.execute_input":"2025-10-13T12:02:33.852126Z","iopub.status.idle":"2025-10-13T12:02:33.868256Z","shell.execute_reply.started":"2025-10-13T12:02:33.8521Z","shell.execute_reply":"2025-10-13T12:02:33.867651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClipCapModel(nn.Module):\n    def __init__(self, clip_model, gpt2_model, prefix_length=10):\n        super().__init__()\n        self.gpt2 = gpt2_model\n        self.clip = clip_model\n        \n        clip_embedding_dim = clip_model.visual.output_dim\n        gpt_embedding_dim = gpt2_model.config.hidden_size\n        \n        self.mapping_network = MappingNetwork(clip_embedding_dim, gpt_embedding_dim, prefix_length=prefix_length)\n\n    def forward(self, image_features, caption_tokens):\n     \n        caption_embeddings = self.gpt2.transformer.wte(caption_tokens['input_ids'].squeeze(1))\n        \n        with torch.no_grad():\n            image_embeddings = self.clip.encode_image(image_features).float()\n            \n        prefix_embeddings = self.mapping_network(image_embeddings)\n        \n        combined_embeddings = torch.cat([prefix_embeddings, caption_embeddings], dim=1) # (16, 10, 768) + (16, 40, 768)\n        \n        prefix_length = self.mapping_network.prefix_length\n        ignore_labels = torch.full((prefix_embeddings.shape[0], prefix_length), -100, device=device)\n        labels = torch.cat([ignore_labels, caption_tokens['input_ids'].squeeze(1)], dim=1) # (16, 10) + (16, 40)\n        \n        prefix_mask = torch.ones(prefix_embeddings.shape[0], prefix_embeddings.shape[1], device=device)\n        combined_mask = torch.cat([prefix_mask, caption_tokens['attention_mask'].squeeze(1)], dim=1) # (16, 10) + (16, 40)\n        \n        outputs = self.gpt2(inputs_embeds=combined_embeddings, attention_mask=combined_mask, labels=labels)\n        \n        return outputs.loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:02:33.868946Z","iopub.execute_input":"2025-10-13T12:02:33.869609Z","iopub.status.idle":"2025-10-13T12:02:33.886124Z","shell.execute_reply.started":"2025-10-13T12:02:33.869581Z","shell.execute_reply":"2025-10-13T12:02:33.885519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nEPOCHS = 30\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-5\nWEIGHT_DECAY = 1e-5\n\ntrain_dataset = FlickrDataset(train_images, image_to_captions, tokenizer, preprocess)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16)\n\nval_dataset = FlickrDataset(val_images, image_to_captions, tokenizer, preprocess, train=False)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=16)\n\nmodel = ClipCapModel(clip_model, gpt2_model).to(device)\n\noptimizer = AdamW(model.mapping_network.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\nfor epoch in range(EPOCHS):\n    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n    \n    model.train()\n    total_train_loss = 0\n    for images, captions, _ in tqdm(train_dataloader, desc=\"Training\"):\n        images = images.to(device)\n        captions = {key: val.to(device) for key, val in captions.items()}\n        \n        optimizer.zero_grad()\n        \n        loss = model(images, captions)\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n    avg_train_loss = total_train_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n    \n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for images, captions, _ in tqdm(val_dataloader, desc=\"Validating\"):\n            images = images.to(device)\n            captions = {key: val.to(device) for key, val in captions.items()}\n            \n            loss = model(images, captions)\n            total_val_loss += loss.item()\n            \n    avg_val_loss = total_val_loss / len(val_dataloader)\n    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n\n    torch.save(model.mapping_network.state_dict(), f\"mapping_network_epoch_{epoch+1}.pth\")\n\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:44:44.736387Z","iopub.execute_input":"2025-10-13T12:44:44.737178Z","iopub.status.idle":"2025-10-13T13:16:03.024678Z","shell.execute_reply.started":"2025-10-13T12:44:44.73715Z","shell.execute_reply":"2025-10-13T13:16:03.023814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(image_path, model, max_length=40, num_beams=5):\n    model.eval()\n    \n    image = Image.open(image_path).convert(\"RGB\")\n    image_processed = preprocess(image).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        image_embedding = clip_model.encode_image(image_processed).float()\n        prefix_embeddings = model.mapping_network(image_embedding)\n        \n        output_ids = gpt2_model.generate(\n            inputs_embeds=prefix_embeddings,\n            max_length=max_length,\n            num_beams=num_beams,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            early_stopping=True\n        )\n        \n        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    return caption\n\nmodel.mapping_network.load_state_dict(torch.load(f\"mapping_network_epoch_{30}.pth\"))\nprint(\"Loaded best mapping network weights for inference.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:17:51.487207Z","iopub.execute_input":"2025-10-13T13:17:51.487504Z","iopub.status.idle":"2025-10-13T13:17:51.652413Z","shell.execute_reply.started":"2025-10-13T13:17:51.487456Z","shell.execute_reply":"2025-10-13T13:17:51.651619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\ntest_sample_keys = random.sample(test_images, 5)\n\nfor image_key in test_sample_keys:\n    image_path = os.path.join(IMAGE_DIR, image_key)\n    \n    generated_caption = generate_caption(image_path, model)\n    \n    image = Image.open(image_path)\n    plt.imshow(image)\n    plt.title(f\"Generated: {generated_caption}\\nGround Truth 1: {image_to_captions[image_key][0]}\")\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:17:53.249052Z","iopub.execute_input":"2025-10-13T13:17:53.249641Z","iopub.status.idle":"2025-10-13T13:17:55.023299Z","shell.execute_reply.started":"2025-10-13T13:17:53.249617Z","shell.execute_reply":"2025-10-13T13:17:55.02257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption_batch(image_batch, model, max_length=40, num_beams=5):\n    \"\"\"\n    Sinh ch√∫ th√≠ch cho m·ªôt batch ·∫£nh ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω.\n    \"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        image_embeddings = clip_model.encode_image(image_batch).float()\n        prefix_embeddings = model.mapping_network(image_embeddings)\n\n        output_ids = gpt2_model.generate(\n            inputs_embeds=prefix_embeddings,\n            max_length=max_length,\n            num_beams=num_beams,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            early_stopping=True\n        )\n        \n        # Decode c·∫£ batch output v√† tr·∫£ v·ªÅ m·ªôt list c√°c caption\n        captions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    \n    return captions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:21.585912Z","iopub.execute_input":"2025-10-13T13:18:21.586411Z","iopub.status.idle":"2025-10-13T13:18:21.591818Z","shell.execute_reply.started":"2025-10-13T13:18:21.586389Z","shell.execute_reply":"2025-10-13T13:18:21.591167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q git+https://github.com/salaniz/pycocoevalcap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:23:26.481952Z","iopub.execute_input":"2025-10-13T12:23:26.482175Z","iopub.status.idle":"2025-10-13T12:23:44.756678Z","shell.execute_reply.started":"2025-10-13T12:23:26.482158Z","shell.execute_reply":"2025-10-13T12:23:44.75588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nimport os\nfrom PIL import Image\n\nmodel = ClipCapModel(clip_model, gpt2_model).to(device)\nmodel.mapping_network.load_state_dict(torch.load(f\"mapping_network_epoch_{30}.pth\"))\n\nmodel.eval()\ngts = {}  # Ground truth captions\nres = {}  # Model-generated captions\n\nimage_to_captions_val = {key: image_to_captions[key] for key in val_images}\n\nfor images, labels, image_paths in tqdm(val_dataloader, desc=\"Evaluating on validation set\"):\n    images = images.to(device)\n    gen_captions = generate_caption_batch(images, model)\n\n    for j, image_path in enumerate(image_paths):\n        image_key = os.path.basename(image_path)\n\n        # 1. L∆∞u ch√∫ th√≠ch do m√¥ h√¨nh sinh ra\n        res[image_key] = [gen_captions[j].strip()]\n        \n        # 2. L·∫•y T·∫§T C·∫¢ ch√∫ th√≠ch m·∫´u t·ª´ dictionary g·ªëc\n        if image_key in image_to_captions_val:\n            gts[image_key] = image_to_captions_val[image_key]\n\ncider_scorer = Cider()\ncider_score, _ = cider_scorer.compute_score(gts, res)\n\n# --- T√≠nh SPICE ---\nspice_scorer = Spice()\nspice_score, _ = spice_scorer.compute_score(gts, res)\n\nprint(f\"üîπ CIDEr: {cider_score:.4f}\")\nprint(f\"üî∏ SPICE: {spice_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:43.81839Z","iopub.execute_input":"2025-10-13T13:18:43.819216Z","iopub.status.idle":"2025-10-13T13:19:30.688805Z","shell.execute_reply.started":"2025-10-13T13:18:43.819184Z","shell.execute_reply":"2025-10-13T13:19:30.687902Z"}},"outputs":[],"execution_count":null}]}